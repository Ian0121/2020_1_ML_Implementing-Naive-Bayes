{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2015314036-project2.ipynb","provenance":[],"authorship_tag":"ABX9TyO+cL9s4/eHaUi7uk+4w7Rs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-fvunjaTzTmw","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H05ftzdMzb-4","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcMYJoRS2l9f","colab_type":"code","colab":{}},"source":["from nltk.corpus import  names\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HG-_YNvR4Dq-","colab_type":"code","colab":{}},"source":["# remove number and punctuation \n","def letters_only(word):\n","  return word.isalpha()\n","\n","\n","# remove name entity\n","from nltk.corpus import names\n","all_names = set(names.words())\n","\n","# lancaster stemmer\n","from nltk.stem import LancasterStemmer\n","ls = LancasterStemmer()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdrTg181idFf","colab_type":"code","colab":{}},"source":["# lancaster stemming\n","# 대문자를 소문자로 변환\n","# 숫자, 기호, 이름 제거\n","\n","def clean_text(doc):\n","  cleaned_doc = []\n","  for word in doc.split(' '): \n","    word = word.lower() # ABD -> abd\n","  \n","    if letters_only(word) and word not in all_names and len(word) > 2: # remove number and punc. and name entity\n","      cleaned_doc.append(ls.stem(word))\n","  return ' '.join(cleaned_doc) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbeBWhtLB19Y","colab_type":"code","colab":{}},"source":["# stopword 제거\n","\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","\n","stop_words = set(stopwords.words('english'))\n","stop_words.update(['arent', 'didnt', 'couldnt', 'cant', 'doesnt', 'dont', 'hes', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt',\n","                   'mustnt', 'neednt', 'shant', 'shes', 'shouldve', 'shouldnt', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', \n","                   'youd', 'youll', 'youre', 'youve', 'would', 'maybe', 'might'])\n","\n","def rm_stopwords(doc):\n","  result = []\n","  doc_tokens = word_tokenize(doc)\n","  for w in doc_tokens:\n","    if w not in stop_words:\n","      result.append(w)\n","  return ' '.join(result)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sdYGu8qSy2Q","colab_type":"code","colab":{}},"source":["# count_words 함수 정의\n","# tuple 데이터를 변수로 받음\n","# 각 word가 5 가지 classes(attempt, behavior, ideation, indicator, supportive)에서 몇 번 나오는지 세는 함수\n","# 출력은 dictrionary\n","def count_words(training_set):\n","  counts_dict = {}\n","  for post, labelnumber in training_set:\n","    for word in word_tokenize(post):\n","      if word not in counts_dict:\n","        counts_dict[word] = [0, 0, 0, 0, 0]\n","      if labelnumber == 0:\n","        counts_dict[word][0] += 1\n","      elif labelnumber == 1:\n","        counts_dict[word][1] += 1\n","      elif labelnumber == 2:\n","        counts_dict[word][2] += 1\n","      elif labelnumber == 3:\n","        counts_dict[word][3] += 1\n","      else:\n","        counts_dict[word][4] += 1\n","  return counts_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dz4gWS025fo0","colab_type":"code","colab":{}},"source":["# classifier 개발\n","\n","def classifier(test_set, train_set):\n","  ##### test set 전 처리\n","  cleaned_test1 = [clean_text(post) for post in test_set.Post]\n","  cleaned_test2 = [rm_stopwords(post) for post in cleaned_test1]\n","  test_set.Post = cleaned_test2\n","\n","  #### train set 전 처리\n","  cleaned_train1 = [clean_text(post) for post in train_set.Post]\n","  cleaned_train2 = [rm_stopwords(post) for post in cleaned_train1]\n","  train_set.Post = cleaned_train2\n","  subset = train_set[['Post', 'LabelNumber']]\n","  tuples = [tuple(x) for x in subset.values]\n","  data = tuples\n","\n","  ##### get prior\n","  # 각 단어 별로 몇 번 나왔는지 확인\n","  word_counting = count_words(data)\n","  # piror 확률을 계산하기 위해 각 class 개수 세기\n","  counts_attempt = 0\n","  counts_behavior = 0\n","  counts_ideation = 0\n","  counts_indicator = 0\n","  counts_supportive = 0\n","  \n","  for i in range(0, len(train_set)):\n","    if df.Label[i] == 'Attempt':\n","      counts_attempt += 1\n","    elif df.Label[i] == 'Behavior':\n","      counts_behavior += 1\n","    elif df.Label[i] == 'Ideation':\n","      counts_ideation += 1\n","    elif df.Label[i] == 'Indicator':\n","      counts_indicator += 1\n","    else:\n","      counts_supportive += 1\n","  # 각 class 별 개수를 전체 개수로 나누어 prior 확률 계산\n","  prob_attempt = counts_attempt / len(train_set)\n","  prob_behavior = counts_behavior / len(train_set)\n","  prob_ideation = counts_ideation / len(train_set)\n","  prob_indicator = counts_indicator / len(train_set)\n","  prob_supportive = counts_supportive / len(train_set)\n","  # prior 확률을 list로 만들어줌\n","  prob_prior = [prob_attempt, prob_behavior, prob_ideation, prob_indicator, prob_supportive]\n","\n","\n","  ##### get likelihood\n","  # likelihood 계산하기 위해 train set의 각 class 별 단어 개수 세기\n","  total_counts_byclass = [0, 0, 0, 0, 0]\n","  for word in word_counting:\n","    for i in range(0,5):\n","      total_counts_byclass[i] += word_counting[word][i]\n","\n","  # likelihood 계산\n","  likelihood={}\n","  for word in word_counting:\n","    likelihood[word] = [0, 0, 0, 0, 0]\n","    for i in range(0,5):\n","      likelihood[word][i] = (word_counting[word][i] + 1) / (total_counts_byclass[i] + len(word_counting))\n","  \n","  #### 분류\n","  result = []\n","  na_count = 0\n","  for post in test_set.Post:\n","    log_prior = np.log(prob_prior)\n","    tokenword = word_tokenize(post)\n","    for i in range(0,5):\n","      for w in tokenword:\n","        if w in likelihood:\n","          log_prior[i] = log_prior[i] + np.log(likelihood[w][i]) # log 취해서 더 해줌. 왜냐하면 그대로 두면 값이 너무 작아짐\n","    result.append(np.where(log_prior == max(log_prior))[0][0])  # 각 class 별로 계산된 확률 중 높은 값의 인덱스를 확인\n","  result2 = np.array(result)\n","  return result2\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2Ana_XWsTIC","colab_type":"code","outputId":"cb318de2-11a2-471a-fc6a-412ed2a65685","executionInfo":{"status":"ok","timestamp":1591491557243,"user_tz":-540,"elapsed":937,"user":{"displayName":"LSH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gieu7rww-SkkaEB7kSRFyyiZ8g8BPtY2P2d9P63=s64","userId":"05418537563809568040"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["df = pd.read_csv(\"TRAIN.csv\")\n","df.test = pd.read_csv(\"TEST.csv\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oFUhtoIGABC5","colab_type":"code","outputId":"e1efcc8b-71e6-42e0-ed2d-32b7ef95b111","executionInfo":{"status":"ok","timestamp":1591491566315,"user_tz":-540,"elapsed":8187,"user":{"displayName":"LSH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gieu7rww-SkkaEB7kSRFyyiZ8g8BPtY2P2d9P63=s64","userId":"05418537563809568040"}},"colab":{"base_uri":"https://localhost:8080/","height":140}},"source":["final = classifier(df.test, df)\n","final"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 1, 2, 2, 2, 4, 2, 4, 2, 2, 4,\n","       2, 2, 2, 4, 4, 3, 2, 3, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2,\n","       2, 3, 2, 3, 1, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 4, 2, 2, 2, 2,\n","       2, 2, 3, 2, 2, 3, 4, 2, 4, 4, 4, 2, 2, 4, 3, 3, 4, 4, 2, 2, 2, 2,\n","       2, 2, 4, 2, 3, 4, 2, 3, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 1, 2, 4,\n","       4, 2, 2, 2, 2, 3, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 1, 4, 2,\n","       4, 2, 4, 4, 2, 2, 2, 2, 1, 4, 4, 2, 4, 2, 4, 2, 2, 2])"]},"metadata":{"tags":[]},"execution_count":1151}]},{"cell_type":"code","metadata":{"id":"EXxk_NmHADe2","colab_type":"code","colab":{}},"source":["pd.DataFrame(final).to_csv(\"2015314036.csv\", index=False)"],"execution_count":0,"outputs":[]}]}